{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## IPython extension to reload modules before executing user code.\n",
    "# Autorelad is an IPython extension to reload modules before executing user code.\n",
    "%load_ext autoreload\n",
    "\n",
    "# Reload all modules (except those excluded by %aimport) every time before executing the Python code typed.\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT_DIR: /home/jovyan/projects/djoy4stem/human_nuclean_receptor_activity\n",
      "DATASET_DIR: /home/jovyan/projects/djoy4stem/human_nuclean_receptor_activity/data\n",
      "SHAP Version : 0.40.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "ROOT_DIR = os.sep.join(os.path.abspath('.').split(os.sep)[:-1])\n",
    "sys.path.insert(0, ROOT_DIR)\n",
    "DATASET_DIR = \"{}/data\".format(ROOT_DIR)\n",
    "print(\"ROOT_DIR: {}\".format(ROOT_DIR))\n",
    "print(\"DATASET_DIR: {}\".format(DATASET_DIR))\n",
    "\n",
    "from datetime import datetime\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem.rdMolDescriptors import GetMorganFingerprint\n",
    "from rdkit.Chem import PandasTools, MolFromSmiles, AllChem, MolFromSmiles, Draw, MolToInchiKey, MolToSmiles\n",
    "from rdkit import DataStructs\n",
    "from rdkit.SimDivFilters.rdSimDivPickers import MaxMinPicker\n",
    "\n",
    "import re\n",
    "from sklearn.preprocessing import StandardScaler, FunctionTransformer, scale\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F # contains functional interface of typical operations used for building nn's\n",
    "import torch.optim as optim     # contains optimizers e.g. Adam, SGD\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchmetrics import ConfusionMatrix, Accuracy\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from tqdm import tqdm\n",
    "\n",
    "from lib import utilities\n",
    "\n",
    "import shap\n",
    "\n",
    "torch.set_printoptions(linewidth=120)\n",
    "\n",
    "print(\"SHAP Version : {}\".format(shap.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "We will various packages for the vectorization of SMILES. These will include MolVecGen and SmilesPE, which can be installed using pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: SmilesPE in /home/jovyan/envs/tox_21_qsar_modeling/lib/python3.7/site-packages (0.0.3)\n",
      "Requirement already satisfied: fastprogress in /home/jovyan/envs/tox_21_qsar_modeling/lib/python3.7/site-packages (from SmilesPE) (1.0.3)\n",
      "Requirement already satisfied: gensim in /home/jovyan/envs/tox_21_qsar_modeling/lib/python3.7/site-packages (from SmilesPE) (4.2.0)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /home/jovyan/envs/tox_21_qsar_modeling/lib/python3.7/site-packages (from gensim->SmilesPE) (1.21.5)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /home/jovyan/envs/tox_21_qsar_modeling/lib/python3.7/site-packages (from gensim->SmilesPE) (6.0.0)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /home/jovyan/envs/tox_21_qsar_modeling/lib/python3.7/site-packages (from gensim->SmilesPE) (1.7.3)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install SmilesPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: deepsmiles in /home/jovyan/envs/tox_21_qsar_modeling/lib/python3.7/site-packages (1.0.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade deepsmiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "from SmilesPE.tokenizer import *\n",
    "from SmilesPE.pretokenizer import atomwise_tokenizer, kmer_tokenizer\n",
    "import deepsmiles\n",
    "spe_vob= codecs.open('/home/jovyan/projects/data/BPE_codes.txt')\n",
    "spe = SPE_Tokenizer(spe_vob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SMILESMolDataset(Dataset):\n",
    "    def __init__(self, smiles, target):\n",
    "        self.smiles = smiles\n",
    "        self.target = target\n",
    "    def __len__(self):\n",
    "        return len(self.smiles)\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "             \n",
    "        sample = self.smiles[idx]\n",
    "        label  = self.target[idx]\n",
    "\n",
    "        return label, sample\n",
    "    \n",
    "# def build_vocab(datasets, my_tokenizer):   \n",
    "#     vocab = set()\n",
    "#     for dataset in datasets:       \n",
    "#         for item in dataset:\n",
    "#             smiles_string, label = item\n",
    "# #             if callable(my_tokenizer):\n",
    "#             tkns = None\n",
    "# #               if my_tokenizer == atomwise_tokenizer:\n",
    "# #                 tkns = atomwise_tokenizer(smiles_string)\n",
    "#             if callable(my_tokenizer):\n",
    "#                 tkns = my_tokenizer(smiles_string)\n",
    "#             elif isinstance(my_tokenizer, SPE_Tokenizer):\n",
    "#                 tkns  = spe.tokenize(smiles_string).split(' ')\n",
    "#             vocab = vocab.union(set(tkns))\n",
    "    \n",
    "#     vocab_encoder = LabelEncoderExt()   \n",
    "#     vocab_encoder = vocab_encoder.fit(list(vocab))     \n",
    "#     return vocab, vocab_encoder\n",
    "\n",
    "def tokenize(smiles, tokenizer_type='spe_tokenizer', ngram=2):\n",
    "    tokens = None\n",
    "    if tokenizer_type=='spe_tokenizer':\n",
    "        tokens = spe.tokenize(smiles).split(' ')\n",
    "    elif tokenizer_type=='kmer_tokenizer':\n",
    "        tokens = kmer_tokenizer(smiles, ngram)\n",
    "    elif tokenizer_type=='atomwise_tokenizer':\n",
    "        tokens = atomwise_tokenizer(smiles)  \n",
    "    return tokens\n",
    "\n",
    "def build_vocab_torchtext(datasets, tokenizer_type='spe_tokenizer', ngram=8):\n",
    "    for dataset in datasets:\n",
    "        for _, smiles in dataset:\n",
    "            yield tokenize(smiles, tokenizer_type=tokenizer_type, ngram=ngram)\n",
    "            \n",
    "def return_binary_class_labels(predictions, threshold, pos_label=1):\n",
    "    return (predictions>threshold).int()*pos_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((9, 'CN(C)c1ccc(, cc1), C(c1ccccc1), =C1, C=CC(, C=, C1), =[N+], (C)C'),\n",
       " (44,\n",
       "  'CN, N(, (C, C), )c, c1, 1c, cc, cc, c(, (c, cc, c1, 1), )C, C(, (c, c1, 1c, cc, cc, cc, cc, c1, 1), )=, =C, C1, 1C, C=, =C, CC, C(, (C, C=, =C, C1, 1), )=, =[N+], [N+](, (C, C), )C'),\n",
       " (45,\n",
       "  'C, N, (, C, ), c, 1, c, c, c, (, c, c, 1, ), C, (, c, 1, c, c, c, c, c, 1, ), =, C, 1, C, =, C, C, (, C, =, C, 1, ), =, [N+], (, C, ), C'))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spe_tkns  = tokenize(\"CN(C)c1ccc(cc1)C(c1ccccc1)=C1C=CC(C=C1)=[N+](C)C\", tokenizer_type='spe_tokenizer')\n",
    "kmer_tkns = tokenize(\"CN(C)c1ccc(cc1)C(c1ccccc1)=C1C=CC(C=C1)=[N+](C)C\", tokenizer_type='kmer_tokenizer', ngram=2)\n",
    "atm_tkns  = tokenize(\"CN(C)c1ccc(cc1)C(c1ccccc1)=C1C=CC(C=C1)=[N+](C)C\", tokenizer_type='atomwise_tokenizer')\n",
    "(len(spe_tkns), ', '.join(spe_tkns)), (len(kmer_tkns), ', '.join(kmer_tkns)), (len(atm_tkns), ', '.join(atm_tkns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "molecule_column = 'Molecule'\n",
    "smiles_column   = \"SMILES\"\n",
    "inchikey_column = \"InChIKey\"\n",
    "target          = \"Activity_Flag\"\n",
    "canonicalize_first = False\n",
    "use_deepsmiles     = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((11821, 11),\n",
       "   Original_Entry_ID Activity_Flag\n",
       " 0          44573832             A\n",
       " 1              6237             N\n",
       " 2           1319563             N\n",
       " 3              8573             N\n",
       " 4              5897             N)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compounds_df = pd.read_csv(\"{}/human_nr1h2_nr1h3_nr1h4.csv\".format(DATASET_DIR), sep=\",\")\n",
    "compounds_df.shape, compounds_df[[\"Original_Entry_ID\", target]].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "if canonicalize_first:\n",
    "    print(compounds_df[[\"Original_Entry_ID\", smiles_column]].head(5))\n",
    "    compounds_df[smiles_column] = compounds_df[smiles_column].apply(lambda x : MolToSmiles( MolFromSmiles(x), canonical=False, kekuleSmiles=False, doRandom=False ))\n",
    "    print(compounds_df[[\"Original_Entry_ID\", smiles_column]].head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RDKit ERROR: [19:58:11] Explicit valence for atom # 12 N, 5, is greater than permitted\n",
      "[19:58:11] Explicit valence for atom # 12 N, 5, is greater than permitted\n",
      "RDKit ERROR: [19:58:14] Explicit valence for atom # 1 N, 5, is greater than permitted\n",
      "[19:58:14] Explicit valence for atom # 1 N, 5, is greater than permitted\n",
      "RDKit ERROR: [19:58:16] Explicit valence for atom # 25 N, 4, is greater than permitted\n",
      "[19:58:16] Explicit valence for atom # 25 N, 4, is greater than permitted\n",
      "RDKit ERROR: [19:58:18] Explicit valence for atom # 1 N, 5, is greater than permitted\n",
      "[19:58:18] Explicit valence for atom # 1 N, 5, is greater than permitted\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4,\n",
       "   Original_Entry_ID Activity_Flag\n",
       " 0          44573832             A\n",
       " 1              6237             N\n",
       " 2           1319563             N\n",
       " 3              8573             N\n",
       " 4              5897             N,\n",
       " ['A', 'N'])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PandasTools.AddMoleculeColumnToFrame(compounds_df,'SMILES',molecule_column, includeFingerprints=True)\n",
    "compounds_df.Molecule.isna().sum(), compounds_df[[\"Original_Entry_ID\", target]].head(5),compounds_df[target].unique().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing invalid (nan) molecules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape before: (11821, 12)\n",
      "Shape after: (11817, 12)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape before: {}\".format(compounds_df.shape))\n",
    "compounds_df = compounds_df.dropna(axis=0, subset=[molecule_column])\n",
    "print(\"Shape after: {}\".format(compounds_df.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'pandas.io.formats.format' has no attribute '_get_adjustment'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/home/jovyan/envs/tox_21_qsar_modeling/lib/python3.7/site-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jovyan/envs/tox_21_qsar_modeling/lib/python3.7/site-packages/rdkit/Chem/PandasTools.py\u001b[0m in \u001b[0;36mpatchPandasrepr\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m   \u001b[0mdefHTMLFormatter_write_cell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhtml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHTMLFormatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_write_cell\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m   \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhtml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHTMLFormatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_write_cell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_patched_HTMLFormatter_write_cell\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m   \u001b[0mdefPandasGetAdjustment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_adjustment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m   \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_adjustment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_patched_get_adjustment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m   \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefPandasRepr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'pandas.io.formats.format' has no attribute '_get_adjustment'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                Ambit_InchiKey Original_Entry_ID  Entrez_ID Activity_Flag  \\\n",
       "0  CZCGWZPRRHCUID-GSBWGOPVNA-N          44573832       9971             A   \n",
       "1  CZCHIEJNWPNBDE-UHFFFAOYNA-N              6237       9971             N   \n",
       "2  CZEFSBFSQCLEDN-PKRZOPRNNA-N           1319563       7376             N   \n",
       "3  CZGIRAWWWHPJHM-YAQRNVERNA-N              8573       9971             N   \n",
       "4  CZIHNRWJTSTCEX-WYUMXYHSNA-N              5897       9971             N   \n",
       "\n",
       "     pXC50                 DB  Original_Assay_ID  Tax_ID Gene_Symbol  \\\n",
       "0  6.14267            pubchem             352765    9606       NR1H4   \n",
       "1      NaN            pubchem             743220    9606       NR1H4   \n",
       "2      NaN  pubchem_screening             743027    9606       NR1H2   \n",
       "3      NaN            pubchem             743220    9606       NR1H4   \n",
       "4      NaN            pubchem             743220    9606       NR1H4   \n",
       "\n",
       "   Ortholog_Group                                             SMILES  \\\n",
       "0            6137  ClC=1C=C(OCC2=C(ON=C2COC3=C(F)C=CC=C3F)C(C)C)C...   \n",
       "1            6137       IC1=CC(C(=O)C=2C=3C(OC2CC)=CC=CC3)=CC(I)=C1O   \n",
       "2            4643  S(=O)(=O)(N(C1=C(C=C(C=C1)C)C)CC(=O)NC2=C(C=3C...   \n",
       "3            6137             O(C(CC)(C)C#C)C(=O)C=1C(=CC=CC1)C(O)=O   \n",
       "4            6137                 O=C(NC1=CC=2CC=3C(C2C=C1)=CC=CC3)C   \n",
       "\n",
       "                                            Molecule  \n",
       "0  <img data-content=\"rdkit/molecule\" src=\"data:i...  \n",
       "1  <img data-content=\"rdkit/molecule\" src=\"data:i...  \n",
       "2  <img data-content=\"rdkit/molecule\" src=\"data:i...  \n",
       "3  <img data-content=\"rdkit/molecule\" src=\"data:i...  \n",
       "4  <img data-content=\"rdkit/molecule\" src=\"data:i...  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compounds_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing duplicates\n",
    "For several compounds (referrenced by Ambit_inchikeys) there are more than one rows. The target values will be averaged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing and storing the InChiKeys...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "if not inchikey_column in compounds_df.columns:\n",
    "    print(\"Computing and storing the InChiKeys...\")\n",
    "    compounds_df[inchikey_column] = compounds_df[molecule_column].apply(lambda x: MolToInchiKey(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "InchIKey BXLSIUXUNIEEJB-UHFFFAOYSA-N has 2 conflicting Activity_Flag values. All associated samples will be removed.\n",
      "InchIKey CDFKRQXFXHECQG-UHFFFAOYSA-N has 2 conflicting Activity_Flag values. All associated samples will be removed.\n",
      "InchIKey CHDXIUFTJYWUOY-UHFFFAOYSA-N has 2 conflicting Activity_Flag values. All associated samples will be removed.\n",
      "InchIKey CRWGIKAUSJNKIC-UHFFFAOYSA-N has 2 conflicting Activity_Flag values. All associated samples will be removed.\n",
      "InchIKey CTGIBRGBEVEPID-UHFFFAOYSA-N has 2 conflicting Activity_Flag values. All associated samples will be removed.\n",
      "InchIKey CWBGTRWGHOYBSM-UHFFFAOYSA-N has 2 conflicting Activity_Flag values. All associated samples will be removed.\n",
      "InchIKey CYYVGYOQNIKMRG-UHFFFAOYSA-N has 2 conflicting Activity_Flag values. All associated samples will be removed.\n",
      "InchIKey DFDLNHGSXIIXAJ-UHFFFAOYSA-N has 2 conflicting Activity_Flag values. All associated samples will be removed.\n",
      "InchIKey HBOMLICNUCNMMY-XLPZGREQSA-N has 2 conflicting Activity_Flag values. All associated samples will be removed.\n",
      "InchIKey HMOXHHMJBYTUJK-UHFFFAOYSA-N has 2 conflicting Activity_Flag values. All associated samples will be removed.\n",
      "InchIKey IDEFAXMSWQNUAX-UHFFFAOYSA-N has 2 conflicting Activity_Flag values. All associated samples will be removed.\n",
      "InchIKey JEJWNQZGLAXVKT-UHFFFAOYSA-N has 2 conflicting Activity_Flag values. All associated samples will be removed.\n",
      "InchIKey KBOKCQLFWLAHHN-UHFFFAOYSA-N has 2 conflicting Activity_Flag values. All associated samples will be removed.\n",
      "InchIKey KPPHKUBDCAKMJR-UHFFFAOYSA-N has 2 conflicting Activity_Flag values. All associated samples will be removed.\n",
      "InchIKey MSMQHCNIVJOKRR-UHFFFAOYSA-N has 2 conflicting Activity_Flag values. All associated samples will be removed.\n",
      "InchIKey NLSLEAPXLGIRCU-UHFFFAOYSA-N has 2 conflicting Activity_Flag values. All associated samples will be removed.\n",
      "InchIKey NLXFYNCHNZGZBT-UHFFFAOYSA-N has 2 conflicting Activity_Flag values. All associated samples will be removed.\n",
      "InchIKey NZFZSSRHOHMVFN-UHFFFAOYSA-N has 2 conflicting Activity_Flag values. All associated samples will be removed.\n",
      "InchIKey OAEVJFPPXJHCKV-UHFFFAOYSA-N has 2 conflicting Activity_Flag values. All associated samples will be removed.\n",
      "InchIKey OFQPQJJVXCUPFR-UHFFFAOYSA-N has 2 conflicting Activity_Flag values. All associated samples will be removed.\n",
      "InchIKey ORZCTXQPXYCOAF-UHFFFAOYSA-N has 2 conflicting Activity_Flag values. All associated samples will be removed.\n",
      "InchIKey ORZUZXZETYSZTE-UHFFFAOYSA-N has 2 conflicting Activity_Flag values. All associated samples will be removed.\n",
      "InchIKey OTTVISRUYFBLEW-MREQRLIMSA-N has 2 conflicting Activity_Flag values. All associated samples will be removed.\n",
      "InchIKey POECRYKCKXHVLF-UHFFFAOYSA-N has 2 conflicting Activity_Flag values. All associated samples will be removed.\n",
      "InchIKey RZPAXNJLEKLXNO-GFKLAVDKSA-N has 2 conflicting Activity_Flag values. All associated samples will be removed.\n",
      "InchIKey STQGQHZAVUOBTE-VGBVRHCVSA-N has 2 conflicting Activity_Flag values. All associated samples will be removed.\n",
      "InchIKey SYQXCJLRSLJMRQ-UHFFFAOYSA-N has 2 conflicting Activity_Flag values. All associated samples will be removed.\n",
      "InchIKey TUMZBTITCWKZAT-UHFFFAOYSA-N has 2 conflicting Activity_Flag values. All associated samples will be removed.\n",
      "InchIKey TXVRGWLYTBYDLP-OWOJBTEDSA-N has 2 conflicting Activity_Flag values. All associated samples will be removed.\n",
      "InchIKey UYGWEAIWBIKKSH-UHFFFAOYSA-N has 2 conflicting Activity_Flag values. All associated samples will be removed.\n",
      "InchIKey VEVZSMAEJFVWIL-UHFFFAOYSA-O has 2 conflicting Activity_Flag values. All associated samples will be removed.\n",
      "InchIKey VGQGXKVPVNOLEN-UHFFFAOYSA-N has 2 conflicting Activity_Flag values. All associated samples will be removed.\n",
      "InchIKey VHGMCOKOKWHSEA-UHFFFAOYSA-N has 2 conflicting Activity_Flag values. All associated samples will be removed.\n",
      "InchIKey VOOHWEWPJYDTGC-UHFFFAOYSA-N has 2 conflicting Activity_Flag values. All associated samples will be removed.\n",
      "InchIKey VVKWAMAHKACEKF-UHFFFAOYSA-N has 2 conflicting Activity_Flag values. All associated samples will be removed.\n",
      "InchIKey WUCULNIWKNLKMF-IPQOISQHSA-N has 2 conflicting Activity_Flag values. All associated samples will be removed.\n",
      "InchIKey WZEUOZFLJDPKMG-SNAWJCMRSA-N has 2 conflicting Activity_Flag values. All associated samples will be removed.\n",
      "InchIKey XGSWAPNRKGOSKD-UHFFFAOYSA-N has 2 conflicting Activity_Flag values. All associated samples will be removed.\n",
      "InchIKey XHUGCOUNHPKMDJ-UHFFFAOYSA-N has 2 conflicting Activity_Flag values. All associated samples will be removed.\n",
      "InchIKey XJGFWWJLMVZSIG-UHFFFAOYSA-N has 2 conflicting Activity_Flag values. All associated samples will be removed.\n",
      "InchIKey ZTRCQAWKQLDVDC-UHFFFAOYSA-N has 2 conflicting Activity_Flag values. All associated samples will be removed.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((11205, 13),\n",
       "     Original_Entry_ID Activity_Flag\n",
       " 759           6917728             N\n",
       " 760           6419962             N\n",
       " 761              4929             N\n",
       " 762     CHEMBL3264640             A\n",
       " 763            759280             N)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compounds_df = utilities.remove_conflicting_target_values(compounds_df, target, inchikey_column)\n",
    "compounds_df.shape, compounds_df[[\"Original_Entry_ID\", target]].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing the activity labels from string to integer values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_classes=['Inactive','Active']\n",
    "compounds_df[target] = compounds_df[target].apply(lambda tgt: 1 if tgt =='A' else 0)\n",
    "compounds_df.shape, compounds_df[[\"Original_Entry_ID\", target]].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MinMax Picking Train/Validate/Test Split\n",
    "Reference(s):\n",
    ">- **Picking Diverse Molecules Using Fingerprints (rdkit.SimDivFilters):** https://www.rdkit.org/docs/GettingStartedInPython.html\n",
    ">- **Squonk: RDKit MaxMin Picker:** https://squonk.it/docs/cells/RDKit%20MaxMin%20Picker/\n",
    ">- **Revisting the MaxMinPicker (2017)** http://rdkit.blogspot.com/2017/11/revisting-maxminpicker.html\n",
    ">- **RDKit Blog - MaxMinPicker**: https://github.com/greglandrum/rdkit_blog/blob/master/notebooks/MaxMinPickerRevisited.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting the dataset...\n",
      "8964 - 1120 - 1121\n",
      "Indices (training):8964 - [4672, 720]\n",
      "Indices (validation):1120 - [4850]\n",
      "Indices (test):1121 - [3]\n",
      "Train: (8964, 13) - Validate: (1120, 13) - Test: (1121, 13)\n",
      "Index(['Ambit_InchiKey', 'Original_Entry_ID', 'Entrez_ID', 'Activity_Flag',\n",
      "       'pXC50', 'DB', 'Original_Assay_ID', 'Tax_ID', 'Gene_Symbol',\n",
      "       'Ortholog_Group', 'SMILES', 'Molecule', 'InChIKey'],\n",
      "      dtype='object')\n",
      "CPU times: user 3min 39s, sys: 24.1 ms, total: 3min 39s\n",
      "Wall time: 3min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "compounds_train, compounds_validation, compounds_test = utilities.min_max_train_validate_test_split_df(compounds_df, molecule_column=molecule_column\n",
    "                                                                                                       , inchikey_column=inchikey_column, fp_column=None\n",
    "                                                                                                       , train_valid_ratios=[0.8, 0.1]\n",
    "                                                                                                        , fp_type= \"morgan\", random_state=1, return_indices=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augment Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'pandas.io.formats.format' has no attribute '_get_adjustment'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/home/jovyan/envs/tox_21_qsar_modeling/lib/python3.7/site-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jovyan/envs/tox_21_qsar_modeling/lib/python3.7/site-packages/rdkit/Chem/PandasTools.py\u001b[0m in \u001b[0;36mpatchPandasrepr\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m   \u001b[0mdefHTMLFormatter_write_cell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhtml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHTMLFormatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_write_cell\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m   \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhtml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHTMLFormatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_write_cell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_patched_HTMLFormatter_write_cell\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m   \u001b[0mdefPandasGetAdjustment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_adjustment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m   \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_adjustment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_patched_get_adjustment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m   \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefPandasRepr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'pandas.io.formats.format' has no attribute '_get_adjustment'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                                SMILES  Activity_Flag\n",
       "0      S(C=1N=C(NC(C)C)N=C(NC(C)C)N1)C              0\n",
       "1           CC(C)Nc1nc(nc(SC)n1)NC(C)C              0\n",
       "2  O=C(O)/C=C/C1=CC=C(C=C1)/C=C/C(=O)O              0\n",
       "3       C(C=Cc1ccc(C=CC(=O)O)cc1)(=O)O              0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def augment_data(compounds_df, smiles_column, target_column, n_randomizations=1):  \n",
    "    temp_dfs = []\n",
    "    for index, row in compounds_df.iterrows():\n",
    "        original_smiles = row[smiles_column]\n",
    "        smiles=[original_smiles]\n",
    "        for i in range(n_randomizations):          \n",
    "            smiles.append(utilities.randomize_smiles(original_smiles))\n",
    "#         print(\"SMILES = {}\".format(smiles))\n",
    "        df = pd.DataFrame(list(set(smiles)), columns=[smiles_column])\n",
    "        df[target_column] = row[target_column]\n",
    "#         print(df)\n",
    "        temp_dfs.append(df)\n",
    "    final_df = pd.concat(temp_dfs, axis=0)\n",
    "    final_df = final_df.reset_index(drop=True)\n",
    "    return final_df\n",
    "\n",
    "t = augment_data(compounds_train.iloc[:2,:], smiles_column, target, n_randomizations=1)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8964, 2), (1120, 2), (1121, 2))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_aug, validation_aug, test_aug = augment_data(compounds_train, smiles_column, target, n_randomizations=0)\\\n",
    ", augment_data(compounds_validation, smiles_column, target, n_randomizations=0)\\\n",
    ", augment_data(compounds_test, smiles_column, target, n_randomizations=0)\n",
    "\n",
    "train_aug.shape, validation_aug.shape, test_aug.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_deepsmiles:\n",
    "    print(\"Converting Smiles to DeepSmiles\")\n",
    "    converter = deepsmiles.Converter(rings=True, branches=True)\n",
    "    print(train_aug[[smiles_column]].head(5))\n",
    "    train_aug[\"DeepSmiles\"] = train_aug[smiles_column].apply(lambda smi :  converter.encode(smi))\n",
    "    print(train_aug[[smiles_column, \"DeepSmiles\"]].head(5))\n",
    "    \n",
    "    validation_aug[\"DeepSmiles\"] = validation_aug[smiles_column].apply(lambda smi :  converter.encode(smi))\n",
    "    test_aug[\"DeepSmiles\"] = test_aug[smiles_column].apply(lambda smi :  converter.encode(smi))\n",
    "    \n",
    "    smiles_column = 'DeepSmiles'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating SMILESMolDataset objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset      = SMILESMolDataset(smiles=train_aug[smiles_column].values, target=train_aug[target].values)\n",
    "validation_dataset = SMILESMolDataset(smiles=validation_aug[smiles_column].values, target=validation_aug[target].values)\n",
    "test_dataset       = SMILESMolDataset(smiles=test_aug[smiles_column].values, target=test_aug[target].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, y_train, X_validation, y_validation, X_test, y_test = utilities.random_train_validation_test_split(compounds_df[smiles_column], compounds_df[target], 0.1, 0.1, random_state=42)\n",
    "# # print(X_train.values.tolist())\n",
    "# train_dataset      = SMILESMolDataset(smiles=X_train.values.tolist(), target=y_train.tolist())\n",
    "# validation_dataset = SMILESMolDataset(smiles=X_validation.values.tolist(), target=y_validation.tolist())\n",
    "# test_dataset       = SMILESMolDataset(smiles=X_test.values.tolist(), target=y_test.tolist())\n",
    "# len(train_dataset), len(validation_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a vocabulary\n",
    "We will use torchtext's functionalities including vocab, and asssociated functions.\n",
    "> - https://pytorch.org/text/stable/vocab.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'generator'>\n",
      "['__class__', '__del__', '__delattr__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__name__', '__ne__', '__new__', '__next__', '__qualname__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', 'close', 'gi_code', 'gi_frame', 'gi_running', 'gi_yieldfrom', 'send', 'throw']\n",
      "['S', '(', 'C', '=', '1', 'N', '=', 'C', '(', 'N', 'C', '(', 'C', ')', 'C', ')', 'N', '=', 'C', '(', 'N', 'C', '(', 'C', ')', 'C', ')', 'N', '1', ')', 'C']\n",
      "CPU times: user 172 ms, sys: 60.1 ms, total: 232 ms\n",
      "Wall time: 232 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "TOKENIZER_TYPE = 'atomwise_tokenizer'\n",
    "N_GRAM = 2\n",
    "\n",
    "tokens = build_vocab_torchtext([train_dataset, test_dataset], tokenizer_type=TOKENIZER_TYPE, ngram = N_GRAM)\n",
    "print(tokens.__class__)\n",
    "print(dir(tokens))\n",
    "for t in tokens:\n",
    "    print(t)\n",
    "    break\n",
    "\n",
    "\n",
    "tt_vocab  = build_vocab_from_iterator(tokens\n",
    "                                      , min_freq=1  # indicates that we'll keep all words whose word frequency is at least min_freq.\n",
    "                                      , specials=[\"<UNK>\"]\n",
    "                                     )\n",
    "tt_vocab.set_default_index(tt_vocab[\"<UNK>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S(C=1N=C(NC(C)C)N=C(NC(C)C)N1)C\n",
      "['S(', 'C=', '1', 'N=C(N', 'C(C)C)', 'N=C(N', 'C(C)C)', 'N1)', 'C']\n",
      "[0, 0, 7, 0, 0, 0, 0, 0, 1]\n",
      "37\n"
     ]
    }
   ],
   "source": [
    "# print(dir(tt_vocab))\n",
    "# print(tt_vocab.get_itos())\n",
    "# print(tt_vocab.get_stoi())\n",
    "print(train_dataset[0][1])\n",
    "print(spe.tokenize(train_dataset[0][1]).split())\n",
    "print(tt_vocab(spe.tokenize(train_dataset[0][1]).split()))\n",
    "print(len(tt_vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch work with datasets and dataloaders to feed the minibatches to the model during training, so we convert the data. Itâ€™s easy to create a dataset from the already created tensors. The batch size is set and the trainloader will shuffle the data when an epoch has been used.\n",
    "Here, we train and validation data loaders that we'll use during the training process to go through data. To create data loaders, we load each dataset again and given them to DataLoader() constructor as input. In order to tokenize and vectorize text documents, we use a helper function that is given to collate_fn argument of the DataLoader() constructor. This function will be applied to each batch when we loop through data using data loaders. The function loops through each text document, tokenize them, and then vectorizes tokens/words using vocabulary. We will keep a maximum number of words per document, denoted by the variable ***max_tokens***. To handle that condition, we have truncated words from documents that have more than 50 words and padded documents (with zeros) to documents that have less than ***max_tokens*** words. The number of words to keep per document is one of the hyperparameters to train. We have kept it at ***max_tokens*** but different values can be tried to check whether any helps improve the accuracy of the model.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tokens=len(tt_vocab)\n",
    "batch_size=128\n",
    "# copied from https://coderzcolumn.com/tutorials/artificial-intelligence/word-embeddings-for-pytorch-text-classification-networks\n",
    "def vectorize_batch(batch):\n",
    "    Y, X = list(zip(*batch))\n",
    "    X = [tt_vocab(spe.tokenize(sample).split(' ')) for sample in X]\n",
    "    X = [sample+([0]* (max_tokens-len(sample))) if len(sample)<max_tokens else sample[:max_tokens] for sample in X] ## Bringing all samples to max_tokens length.\n",
    "    return torch.tensor(X), torch.tensor(Y) ## We have deducted 1 from target names to get them in range [0,1,2,3,5] from [1,2,3,4,5]\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, collate_fn=vectorize_batch)\n",
    "validation_loader  = DataLoader(validation_dataset, batch_size=batch_size, collate_fn=vectorize_batch)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=batch_size, collate_fn=vectorize_batch)\n",
    "\n",
    "# print(\"TRAIN ------------\")\n",
    "# for X, Y in train_loader:\n",
    "#     print(X)\n",
    "#     print(Y)\n",
    "#     break\n",
    "# print(\"\\nVALIDATION ------------\")\n",
    "# for X, Y in validation_loader:\n",
    "#     print(X)\n",
    "#     print(Y)\n",
    "#     break\n",
    "# print(\"\\nTEST ------------\")\n",
    "# for X, Y in test_loader:\n",
    "#     print(X)\n",
    "#     print(Y)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "If you run into <em>\"RuntimeError: CUDA error: device-side assert triggered\"</em>, it is possible that the notebook  is running out of GPU memory. This can sometimes happen when you rerun some of the cells but the memory  is not properly released beforehand. \n",
    "1. Try using the CPU to see if the problem still exists\n",
    "2. Check your labels, i.e.: \"whether -1 exists in the labels of the training data\"\n",
    "\n",
    "References:\n",
    ">- https://pycad.co/runtimeerror-cuda-error-device-side-assert-triggered/\n",
    ">- https://github.com/deepset-ai/haystack/issues/1833\n",
    ">- https://clay-atlas.com/us/blog/2021/06/23/linux-en-machine-learning-pytorch-cuda-error-device-side-assert/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning\n",
    "<strong>Loss functions</strong> \\\n",
    "We will use two different loss functions, namely, the BCE Loss and the BCEWithLogitsLoss functions. The difference between nn.BCEWithLogitsLoss and nn.BCELoss is that BCE with Logits loss adds the Sigmoid function into the loss function. With simple BCE Loss, you will have to add Sigmoid to the neural network, whereas with BCE With Logits Loss you will not. The BCEWithLogitsLoss is generally more stable and thus recommended.\n",
    "\n",
    ">- https://github.com/christianversloot/machine-learning-articles/blob/main/how-to-use-pytorch-loss-functions.md\n",
    ">- https://en.wikipedia.org/wiki/LogSumExp#log-sum-exp_trick_for_log-domain_calculations\n",
    "\n",
    "**Statistics report** \\\n",
    "We will compute the confusion matrix and accuracy on the train/validate/test sets.\n",
    ">- https://torchmetrics.readthedocs.io/en/stable/classification/confusion_matrix.html\n",
    ">- https://torchmetrics.readthedocs.io/en/stable/classification/accuracy.html\n",
    "\n",
    "**Early stopping and learning rate scheduler** <!-- No \\ symbol after, since there is no sentence right after-->\n",
    "\n",
    ">- https://debuggercafe.com/using-learning-rate-scheduler-and-early-stopping-with-pytorch/\n",
    ">- https://pythonguides.com/pytorch-early-stopping/\n",
    ">- https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.ReduceLROnPlateau.html#torch.optim.lr_scheduler.ReduceLROnPlateau\n",
    "\n",
    "A helpful library for implementing LR and ES is torchsample, which can be found on [GitHub](https://github.com/ncullen93/torchsample). An example of how to use it is described [here](https://medium.com/analytics-vidhya/early-stopping-with-pytorch-to-restrain-your-model-from-overfitting-dce6de4081c5). But we will use a much simpler implementation.\n",
    "\n",
    "P.S.: Instead of using early stopping or schedule the learning rate, one can use optimizaers such as Adam that already implements a learning rate scheduling approach. However, it does not always outperforms. However, it can cause model divergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping():\n",
    "    \"\"\"\n",
    "    Early stopping to stop the training when the loss does not improve after\n",
    "    certain epochs. Code written by Sovit Ranjan RathSovit Ranjan Rath \n",
    "    https://colab.research.google.com/drive/1krRZ-VVfpXUsHk3JFCjBtYUiUOwL8vW0?usp=sharing&pli=1&authuser=1#scrollTo=TBuoZ1PgK01f\n",
    "    \"\"\"\n",
    "    def __init__(self, patience=5, min_delta=0):\n",
    "        \"\"\"\n",
    "        :param patience: how many epochs to wait before stopping when loss is\n",
    "               not improving\n",
    "        :param min_delta: minimum difference between new loss and old loss for\n",
    "               new loss to be considered as an improvement\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss == None:\n",
    "            self.best_loss = val_loss\n",
    "        elif self.best_loss - val_loss > self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            # reset counter if validation loss improves\n",
    "            self.counter = 0\n",
    "        elif self.best_loss - val_loss < self.min_delta:\n",
    "            self.counter += 1\n",
    "            print(f\"INFO: Early stopping counter {self.counter} of {self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                print('INFO: Early stopping')\n",
    "                self.early_stop = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivityNet(nn.Module):\n",
    "    def __init__(self, num_tokens, embedding_size, hidden_size, dropout_rate, output_size, squash_output=True):\n",
    "        super(ActivityNet, self).__init__()\n",
    "        \n",
    "        self.num_tokens     = num_tokens          \n",
    "        self.hidden_size    = hidden_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.dropout_rate   = dropout_rate\n",
    "        self.output_size    = output_size\n",
    "        self.squash_output  = squash_output\n",
    "        # print(\"{} - {}\".format(self.num_tokens, self.embedding_size)). \n",
    "        \n",
    "        ## Embedding Layer \n",
    "        # We intitialize a layer with a number of embeddings equal to the length of vocabulary and embedding length embedding_size.\n",
    "        # This initialization will create a weight tensor of shape (num_tokens, embedding_size) which has an embedding vector of \n",
    "        # length embedding_size for each token of vocabulary.\n",
    "        # The layer is responsible for mapping the index (a number in the range [0, num_tokens]) of each token to a float vector \n",
    "        # of length embedding_size because we have set the embedding length to embedding_size. \n",
    "        # This layer takes tensor of shape (batch_size, max_tokens) and outputs tensor of shape (batch_size, max_tokens, embed_len).\n",
    "        # Although \"max_tokens is not defined here, each smiles strings in this code was set to a maximum of max_length tokens\"\n",
    "        # Each token gets assigned its respective embedding vector based on the index value by this layer.\n",
    "\n",
    "        self.embedding = nn.Embedding(num_embeddings=self.num_tokens, embedding_dim=self.embedding_size)\n",
    "        \n",
    "        ## the LSTM Layer\n",
    "        # The LSTM layer internally loops through embeddings of each text example and generates hidden and output tensors. The layer is basically a\n",
    "        # loop through embeddings of single text examples one by one generating output after each token. It takes the hidden state and the carry\n",
    "        # as input which are generally random numbers and required by first token only. The input to LSTM is of shape (batch_size, max_tokens, embedding_size).\n",
    "        # Given that we are using only one LSTM layer, the output is of shape (batch_size, max_tokens, hidden_size).\n",
    "        #\n",
    "        # LSTM.weight_ih_l[k] â€“ the learnable input-hidden weights of the k-th layer (W_ii|W_if|W_ig|W_io), of shape (4*hidden_size, input_size)\n",
    "        # LSTM.weight_hh_l[k] â€“ the learnable hidden-hidden weights of the k-th layer (W_ii|W_if|W_ig|W_io), of shape (4*hidden_size, input_size)\n",
    "        # LSTM.bias_ih_l[k] â€“ the learnable input-hidden bias of the k-th layer (b_ii|b_if|b_ig|b_io), of shape (4*hidden_size) \n",
    "        # LSTM.bias_hh_l[k] â€“ the learnable input-hidden bias of the k-th layer  (b_hi|b_hf|b_hg|b_ho), of shape (4*hidden_size) \n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=self.embedding_size, hidden_size=self.hidden_size, num_layers=1\n",
    "                            , batch_first=True # If True, then the input and output tensors are provided as (batch, seq, feature) instead of (seq, batch, feature).\n",
    "                            , bidirectional=False # If True, becomes a bidirectional LSTM.\n",
    "                            )\n",
    "        self.h1 = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "#         self.h2 = nn.Linear(hidden_size, hidden_size)\n",
    "#         self.h3 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.output_layer = nn.Linear(self.hidden_size, self.output_size)\n",
    "      \n",
    "        # Activation\n",
    "        self.hl_activation = nn.ReLU()\n",
    "        if self.squash_output:\n",
    "            self.ol_activation = nn.Sigmoid()\n",
    "        \n",
    "        # Regularization\n",
    "        self.dropout1 = nn.Dropout(self.dropout_rate)\n",
    "        self.dropout2 = nn.Dropout(self.dropout_rate)\n",
    "#         self.dropout3 = nn.Dropout(self.dropout_rate)\n",
    "\n",
    "        for layer in self.children():\n",
    "            print(\"Layer : {}\".format(layer))\n",
    "            print(\"Parameters : \")\n",
    "            # https://blog.paperspace.com/pytorch-101-advanced/\n",
    "            for param in layer.named_parameters():\n",
    "                print(\"{} : {}\".format(param[0], param[1].shape))\n",
    "            print()\n",
    "       \n",
    "    def forward(self, X):        \n",
    "#         print(\"X class = {}\".format(X.__class__))\n",
    "        embeddings = self.embedding(X)\n",
    "#         self.flat(embeddings)\n",
    "#         embeddings = embeddings.mean(dim=1)\n",
    "#         print(embeddings.shape)\n",
    "#         print(\"Mean: {}\".format(embeddings.mean(dim=1).shape))\n",
    "#         print(embeddings.shape)\n",
    "#         print(embeddings)\n",
    "#         h_n, c_n = torch.randn(1, len(X), self.hidden_size).to(device), torch.randn(1, len(X), self.hidden_size).to(device)\n",
    "#         output, (h_n, c_n) = self.lstm(embeddings, (h_n, c_n))\n",
    "        output, (h_n, c_n) = self.lstm(embeddings)\n",
    "#         output, (h_n, c_n) = self.lstm(embeddings)\n",
    "#         print(\"LSTM output:{} h_n:{} c_n:{}\".format(output.shape, h_n.shape, c_n.shape))\n",
    "#         print(self.dropout1)\n",
    "#         output = self.dropout1(h_n) #Dropout\n",
    "        output = self.h1(output) # Pass into the hidden layer\n",
    "#         output = self.hl_activation(output) # Use ReLU on hidden activation\n",
    "        output = self.dropout2(output) # dropout\n",
    "        \n",
    "#         output = self.h2(output)\n",
    "#         output = self.hl_activation(output)\n",
    "#         output = self.dropout3(output)\n",
    "        output = self.output_layer(output)\n",
    "\n",
    "        if self.squash_output:\n",
    "            output = self.ol_activation(output)\n",
    "#         print(\"output shape: {}\".format(output.shape))\n",
    "        output_view = output[:,-1,:].view(-1)\n",
    "#         print(\"output_view shape: {}\".format(output_view.shape))\n",
    "#         print(output_view)\n",
    "#         print(output)\n",
    "        return output_view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(model, criterion, optimizer, train_loader, pos_label=1, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Fit the model, and compute loss and accuracy\n",
    "    \"\"\"\n",
    "    running_loss        = 0\n",
    "    num_samples         = 0\n",
    "    correct_predictions = 0\n",
    "    count = 0\n",
    "    model.train()\n",
    "    for smiles_rep, labels in train_loader:\n",
    "        smiles_rep = smiles_rep.to(device)\n",
    "        labels = labels.to(device)\n",
    "#         print(smiles_rep.shape)\n",
    "#         print(labels.shape)\n",
    "\n",
    "        # Training pass\n",
    "        # (1) Initialize the gradients, which will be recorded during the forward pass\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # (2) Forward pass of the mini-batch\n",
    "#         output = model(smiles_rep)[0].flatten().float()\n",
    "        output = model(smiles_rep).float()\n",
    "#         print(\"Train outputbefore : {}\".format(output.shape))\n",
    "#         output = output[:,-1,:].view(-1)\n",
    "#         print(\"Train output after: {}\".format(output.shape))\n",
    "#         print(\"Train output: {}\".format(output.type()))\n",
    "#         print(\"Train output[:5]: {}\".format(output))\n",
    "#         print(output.view(-1))\n",
    "\n",
    "        # (3) Computing the loss\n",
    "#         print(\"Labels size: {}\".format(labels.shape))\n",
    "#         print(\"Labels: {}\".format(labels.float().type()))\n",
    "#         print(\"Labels: {}\".format(labels))\n",
    "        loss = criterion(output, labels.float())\n",
    "#         print(\"Train Loss: {}\".format(loss))\n",
    "\n",
    "        # (4) Computes the gradient of current tensor w.r.t. graph leaves.\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "        # (5) Optimize the weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss    += loss.item()          \n",
    "        num_samples     += labels.size()[0]\n",
    "        predicted_labels = return_binary_class_labels(output, threshold, pos_label)\n",
    "        correct_predictions += (predicted_labels == labels).sum().item()       \n",
    "\n",
    "#         print(\"Train output     = {}\".format(output))\n",
    "#         print(\"predicted_labels = {}\".format(predicted_labels))\n",
    "#         print(\"Labels           = {}\".format(labels))\n",
    "        \n",
    "    train_loss     = running_loss/len(train_loader)            \n",
    "    train_accuracy = correct_predictions/num_samples\n",
    "        \n",
    "    return  train_loss, train_accuracy\n",
    "\n",
    "\n",
    "def validate_model(model, criterion, optimizer, validation_loader, pos_label=1, threshold=0.5):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    running_loss        = 0\n",
    "    num_val_samples     = 0\n",
    "    correct_predictions = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for smiles_rep, labels in validation_loader:\n",
    "            smiles_rep = smiles_rep.to(device)\n",
    "            labels = labels.to(device)\n",
    "            output = model(smiles_rep)\n",
    "#             output = output[:,-1,:].view(-1)\n",
    "            \n",
    "#             print(\"Validation output: {}\".format(output.shape))\n",
    "#             print(\"Validation: {}\".format(output.type()))\n",
    "#             print(\"Validation[:5]: {}\".format(output[:10]))              \n",
    "#             print(\"Labels: {}\".format(labels.float().type()))\n",
    "#             print(\"Labels: {}\".format(labels[:10]))   \n",
    "\n",
    "            loss = criterion(output, labels.float())\n",
    "            running_loss     += loss\n",
    "#             print(\"Running validation_loss = {}\".format(running_loss))\n",
    "            num_val_samples  += labels.size()[0]\n",
    "            predicted_labels  = return_binary_class_labels(output, threshold, pos_label)\n",
    "#             print(\"Predicted labels ({}) : {}\".format(predicted_labels.shape, predicted_labels[:5]))\n",
    "#             print(\"Labels : {}\".format(labels[:5]))\n",
    "#             print(\"predicted_labels == labels : {}\".format((predicted_.flatten()).sum().item()\n",
    "            correct_predictions += (predicted_labels == labels).sum().item()    \n",
    "        validation_loss     = running_loss/len(validation_loader)\n",
    "#         print(\"validation_loss = {}\".format(validation_loss))\n",
    "#         validation_loss     = (running_loss/len(validation_loader)).to('cpu')            \n",
    "        validation_accuracy = correct_predictions/num_val_samples   \n",
    "        \n",
    "    return validation_loss, validation_accuracy\n",
    "\n",
    "\n",
    "def train_model(model, criterion, optimizer, n_epochs, train_loader, validation_loader, lr_scheduler=None, early_stopper=None, with_proba=True, pos_label=1, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Fit and validate model\n",
    "    \"\"\"\n",
    "    train_losses, validation_losses, train_accuracies, validation_accuracies = [], [], [], []\n",
    "    for e in tqdm(range(n_epochs),desc='Training'):\n",
    "#         print(\"Epoch {} of {} epochs\".format(e+1, n_epochs))\n",
    "        train_loss, train_accuracy           = fit_model(model=model, criterion=criterion, optimizer=optimizer, train_loader=train_loader\n",
    "                                                         , pos_label=pos_label, threshold=threshold)\n",
    "        validation_loss, validation_accuracy = validate_model(model=model, criterion=criterion, optimizer=optimizer, validation_loader=validation_loader\n",
    "                                                              , pos_label=pos_label, threshold=threshold)\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        validation_losses.append(validation_loss.item())\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        validation_accuracies.append(validation_accuracy)\n",
    "        \n",
    "        if lr_scheduler is not None:\n",
    "            lr_scheduler.step(validation_loss)\n",
    "            \n",
    "        if early_stopper is not None:\n",
    "            early_stopper(validation_loss)\n",
    "            if early_stopper.early_stop:\n",
    "                break\n",
    "        \n",
    "        if e%10 == 0:\n",
    "            print(\"Epoch: %3i Training loss: %0.2F Validation loss: %0.2F\"%(e,(train_loss), validation_loss))\n",
    "            print(\"Epoch: %3i Training accuracy: %0.2F Validation accuracy: %0.2F\"%(e,(train_accuracy), validation_accuracy))\n",
    "    \n",
    "    return train_losses, validation_losses, train_accuracies, validation_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs       = 25\n",
    "num_tokens     = len(tt_vocab)  ## setting this to a number lower than len(vocab) will lead to errors, since the indices go from 0 to len(tt_vocab)\n",
    "embedding_size = 25\n",
    "hidden_size    = 128  # The size of the hidden non-linear layer\n",
    "dropout_rate   = 0.2 # The dropout rate\n",
    "output_size    = 1    # This is just a single task, so this will be one\n",
    "\n",
    "learning_rate  = 0.01  # The initial learning rate for the optimizer\n",
    "lr_patience    = 10\n",
    "es_patience    = 50 # Make sure that the patience for early stopping is larger than the learning rate scheduler patience\n",
    "early_stopper  = EarlyStopping(min_delta=0.00001, patience = es_patience)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using BCELoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer : Embedding(37, 25)\n",
      "Parameters : \n",
      "weight : torch.Size([37, 25])\n",
      "\n",
      "Layer : LSTM(25, 128, batch_first=True)\n",
      "Parameters : \n",
      "weight_ih_l0 : torch.Size([512, 25])\n",
      "weight_hh_l0 : torch.Size([512, 128])\n",
      "bias_ih_l0 : torch.Size([512])\n",
      "bias_hh_l0 : torch.Size([512])\n",
      "\n",
      "Layer : Linear(in_features=128, out_features=128, bias=True)\n",
      "Parameters : \n",
      "weight : torch.Size([128, 128])\n",
      "bias : torch.Size([128])\n",
      "\n",
      "Layer : Linear(in_features=128, out_features=1, bias=True)\n",
      "Parameters : \n",
      "weight : torch.Size([1, 128])\n",
      "bias : torch.Size([1])\n",
      "\n",
      "Layer : ReLU()\n",
      "Parameters : \n",
      "\n",
      "Layer : Sigmoid()\n",
      "Parameters : \n",
      "\n",
      "Layer : Dropout(p=0.2, inplace=False)\n",
      "Parameters : \n",
      "\n",
      "Layer : Dropout(p=0.2, inplace=False)\n",
      "Parameters : \n",
      "\n",
      "Number tokens=37\n"
     ]
    }
   ],
   "source": [
    "criterion    = nn.BCELoss(reduction='mean') # De facto for binary classification\n",
    "threshold    = torch.tensor([0.5]).to(device) # Values above the threshold are classified into category pos_label (usually 1) # , and those below 0.5 are classified into value 0\n",
    "model        = ActivityNet(num_tokens, embedding_size, hidden_size, dropout_rate, output_size, squash_output=True).to(device=device) # Returns a Tensor with the specified device\n",
    "optimizer    = optim.Adadelta(model.parameters(), lr=learning_rate)\n",
    "# optimizer    = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "# optimizer    = optim.Atom(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# criterion = nn.BCEWithLogitsLoss()\n",
    "# threshold    = torch.tensor([0.0]).to(device) \n",
    "# model        = ActivityNet(num_tokens, embedding_size, hidden_size, dropout_rate, output_size, squash_output=False).to(device=device) # Returns a Tensor with the specified device\n",
    "# optimizer    = optim.Adadelta(model.parameters(), lr=learning_rate)\n",
    "# print(model)\n",
    "\n",
    "\n",
    "# https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.ReduceLROnPlateau.html#torch.optim.lr_scheduler.ReduceLROnPlateau\n",
    "lr_scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5\n",
    "                                 , patience=lr_patience # Number of epochs with no improvement after which learning rate will be reduced\n",
    "                                 , verbose=True\n",
    "                                 , threshold=0.0001\n",
    "                                 , threshold_mode='rel', cooldown=0, min_lr=learning_rate, eps=1e-08)\n",
    "\n",
    "\n",
    "# # https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.OneCycleLR.html#torch.optim.lr_scheduler.OneCycleLR\n",
    "# lr_scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer, max_lr = 0.004, total_steps = n_epochs*len(train_loader),\n",
    "#                       div_factor=25 # Determines the initial learning rate via initial_lr = max_lr/div_factor Default: 25\n",
    "#                      , final_div_factor=0.08)\n",
    "\n",
    "print(\"Number tokens={}\".format(model.num_tokens))\n",
    "# print(validation_dataset[0].__class__)\n",
    "# model.forward(validation_dataset[0][1])  #, model.forward(X_validation[10:11]), model #, [p for p in model.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   4%|â–         | 1/25 [00:00<00:12,  1.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   0 Training loss: 0.62 Validation loss: 0.66\n",
      "Epoch:   0 Training accuracy: 0.94 Validation accuracy: 0.65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 11/25 [00:04<00:05,  2.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  10 Training loss: 0.22 Validation loss: 1.04\n",
      "Epoch:  10 Training accuracy: 0.94 Validation accuracy: 0.65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 21/25 [00:09<00:01,  2.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  20 Training loss: 0.22 Validation loss: 1.05\n",
      "Epoch:  20 Training accuracy: 0.94 Validation accuracy: 0.65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:10<00:00,  2.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving loss and accuracy plots...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11 s, sys: 120 ms, total: 11.1 s\n",
      "Wall time: 11.1 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6163707425896551"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "train_losses, validation_losses, train_accuracies, validation_accuracies = train_model(model=model, criterion=criterion, optimizer=optimizer, n_epochs=n_epochs\n",
    "                                                                                     , train_loader=train_loader, validation_loader=validation_loader\n",
    "                                                                                     , lr_scheduler=lr_scheduler, early_stopper=None\n",
    "                                                                                     , pos_label=1, threshold=threshold)\n",
    "\n",
    "print('\\nSaving loss and accuracy plots...')\n",
    "# accuracy plots\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.plot(train_accuracies, color='green', label='train accuracy')\n",
    "plt.plot(validation_accuracies, color='blue', label='validataion accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "# plt.legend()\n",
    "plt.savefig(\"{}/figures/Accuracies by epoch\".format(ROOT_DIR))\n",
    "plt.show()\n",
    "\n",
    "# loss plots\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.plot(train_losses, color='orange', label='train loss')\n",
    "plt.plot(validation_losses, color='red', label='validataion loss')\n",
    "# plt.plot([x for x in train_losses], color='orange', label='train loss')\n",
    "# plt.plot([x.to('cpu') for x in validation_losses], color='red', label='validataion loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.savefig(\"{}/figures/Losses by epoch\".format(ROOT_DIR))\n",
    "plt.show()\n",
    "\n",
    "# validation_losses[0].shape, len(validation_losses), train_losses[0].shape, len(train_losses)\n",
    "train_losses[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If the validation loss is generally smaller than the training loss, read the following post:**\n",
    ">[Why is my validation loss lower than my training loss?](https://pyimagesearch.com/2019/10/14/why-is-my-validation-loss-lower-than-my-training-loss/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 37])\n",
      "torch.Size([128, 37])\n",
      "torch.Size([128, 37])\n",
      "torch.Size([128, 37])\n",
      "torch.Size([128, 37])\n",
      "torch.Size([128, 37])\n",
      "torch.Size([128, 37])\n",
      "torch.Size([128, 37])\n",
      "torch.Size([97, 37])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1121,), (1121,))"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import gc\n",
    "\n",
    "def MakePredictions(model, loader):\n",
    "    Y_shuffled, Y_preds = [], []\n",
    "    for X, Y in loader:\n",
    "        X = X.to(device)\n",
    "        Y = Y.to(device)\n",
    "        preds = model(X)\n",
    "        print(X.shape)\n",
    "        pred_classes = return_binary_class_labels(preds, threshold, pos_label=1).flatten()\n",
    "#         print(\"preds = {} | labels = {}\".format(pred_classes.shape, Y.shape))\n",
    "#         print(\"Y = {}\".format(Y))\n",
    "#         print(\"pred_classes = {}\".format(pred_classes))\n",
    "       \n",
    "        Y_preds.append(pred_classes)\n",
    "        Y_shuffled.append(Y)\n",
    "    gc.collect()\n",
    "    Y_preds, Y_shuffled = torch.cat(Y_preds), torch.cat(Y_shuffled)\n",
    "\n",
    "    return Y_shuffled.detach().to('cpu').numpy(), Y_preds.detach().to('cpu').numpy()\n",
    "Y_actual, Y_preds = MakePredictions(model, test_loader)\n",
    "Y_actual.shape, Y_preds.shape #, Y_actual, Y_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy : 0.6610169491525424\n",
      "\n",
      "Classification Report : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Inactive       0.66      1.00      0.80       741\n",
      "      Active       0.00      0.00      0.00       380\n",
      "\n",
      "    accuracy                           0.66      1121\n",
      "   macro avg       0.33      0.50      0.40      1121\n",
      "weighted avg       0.44      0.66      0.53      1121\n",
      "\n",
      "\n",
      "Confusion Matrix : \n",
      "[[741   0]\n",
      " [380   0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "print(\"Test Accuracy : {}\".format(accuracy_score(Y_actual, Y_preds)))\n",
    "print(\"\\nClassification Report : \")\n",
    "print(classification_report(Y_actual, Y_preds, target_names=target_classes))\n",
    "print(\"\\nConfusion Matrix : \")\n",
    "print(confusion_matrix(Y_actual, Y_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import scikitplot as skplt\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "skplt.metrics.plot_confusion_matrix([target_classes[i] for i in Y_actual], [target_classes[i] for i in Y_preds],\n",
    "                                    normalize=True,\n",
    "                                    title=\"Confusion Matrix\",\n",
    "                                    cmap=\"Purples\",\n",
    "                                    hide_zeros=True,\n",
    "                                    figsize=(5,5)\n",
    "                                    );\n",
    "plt.xticks(rotation=90);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explain Predictions Using SHAP Values\n",
    "In this section, we have tried to explain predictions made by our network by generating **SHAP** values using shap python library. In order to use shap library, we need to import it and initialize it by calling **initjs()** function on it which we did at the beginning of the tutorial.\n",
    "\n",
    "In order to explain a prediction using SHAP values, we need to create **Explainer** first. Then, we need to give text examples that we want to explain to the explainer instance to create **Explanation** object (SHAP values). At last, we can call **text_plot()** method by giving SHAP values to it to visualize explanations created for text examples.\n",
    "\n",
    "Below, we have first created an explainer object. The explainer object requires us to provide a function that takes as an input batch of text examples and returns probabilities for each target class for the whole batch. We have created a simple function that takes as an input batch of text samples. It then tokenizes them and creates indexes for tokens using vocabulary. It then assures that each text sample has a length of ***max_tokens*** as required by our network. Then, it gives vectorized batch data to the network to make predictions. As our network returns logits, we have converted them to probabilities using **softmax** activation function. We have also given target class names when creating explainer instances.\n",
    "\n",
    "If you do not have a background on SHAP library then we suggest the below-mentioned tutorials that can be very helpful to learn it.\n",
    "> - [SHAP - Explain Machine Learning Model Predictions using Game-Theoretic Approach](https://coderzcolumn.com/tutorials/machine-learning/shap-explain-machine-learning-model-predictions-using-game-theoretic-approach)\n",
    "> - [SHAP Values for Text Classification Tasks (Keras NLP)](https://coderzcolumn.com/tutorials/artificial-intelligence/shap-values-for-text-classification-tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "##https://shap.readthedocs.io/en/latest/example_notebooks/text_examples/sentiment_analysis/Using%20custom%20functions%20and%20tokenizers.html\n",
    "def custom_tokenizer(smiles, return_offsets_mapping=True):\n",
    "    \"\"\"\n",
    "    This is a tokenizer that will be passed to the shap explainer\n",
    "    \"\"\"\n",
    "    s = ' '.join(tokenize(smiles, tokenizer_type=TOKENIZER_TYPE, ngram=N_GRAM))\n",
    "#     print(\"\\ns={}\".format(s))\n",
    "    pattern = r'\\W'\n",
    "    matches = re.finditer(pattern, s)\n",
    "\n",
    "    pos = 0\n",
    "    offset_ranges = []\n",
    "    input_ids = []\n",
    "\n",
    "    for match in matches:\n",
    "    #     print(match)\n",
    "        start, end = match.span(0)\n",
    "        offset_ranges.append((pos, start))\n",
    "        input_ids.append(s[pos:start])\n",
    "        pos = end    \n",
    "#         print(\"start {} end {} offset_ranges {} input_ids {}\".format(start, end, offset_ranges, input_ids))\n",
    "    if pos != len(s):\n",
    "        offset_ranges.append((pos, len(s)))\n",
    "        input_ids.append(s[pos:])\n",
    "    # print(\"input_ids {}\".format(input_ids))\n",
    "    out = {}\n",
    "    out[\"input_ids\"] = input_ids\n",
    "\n",
    "    if return_offsets_mapping:\n",
    "        out[\"offset_mapping\"] = offset_ranges\n",
    "#     print(out)\n",
    "    return out\n",
    "\n",
    "# custom_tokenizer(\"CCCOCCCc1cnccn1\", return_offsets_mapping=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<shap.explainers._partition.Partition at 0x7fa57c431c50>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_predictions(X_batch_text):\n",
    "#     print(\"X_batch_text class = {}\".format(X_batch_text.__class__))\n",
    "#     X_batch = [vocab(tokenizer(sample)) for sample in X_batch_text]\n",
    "    X_batch = [tt_vocab(spe.tokenize(sample).split(' ')) for sample in X_batch_text]\n",
    "    X_batch = [sample+([0]* (max_tokens-len(sample))) if len(sample)<max_tokens else sample[:max_tokens] for sample in X_batch] ## Bringing all samples to max_tokens.\n",
    "    X_batch = torch.tensor(X_batch).to(device)\n",
    "    y_preds = model(X_batch)\n",
    "    return y_preds.detach().to('cpu')\n",
    "#     return return_binary_class_labels(y_preds, threshold, pos_label=1).flatten().detach().to('cpu')\n",
    "\n",
    "masker    = shap.maskers.Text(custom_tokenizer)\n",
    "explainer = shap.Explainer(make_predictions, masker=masker, output_names=target_classes)\n",
    "explainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual    Target Values : ['Active', 'Inactive', 'Inactive', 'Active', 'Active', 'Inactive', 'Inactive', 'Inactive', 'Inactive', 'Inactive']\n",
      "Predicted Target Values : ['Inactive', 'Inactive', 'Inactive', 'Inactive', 'Inactive', 'Inactive', 'Inactive', 'Inactive', 'Inactive', 'Inactive']\n",
      "Predicted Probabilities : tensor([0.0556, 0.0556, 0.0556, 0.0556, 0.0556, 0.0556, 0.0556, 0.0556, 0.0556, 0.0556])\n"
     ]
    }
   ],
   "source": [
    "X_test, Y_test = [], []\n",
    "for Y, X in test_dataset: ## Selecting first 1024 samples from test data\n",
    "    X_test.append(X)\n",
    "    Y_test.append(Y) ## Please make a Note that we have subtracted 1 from target values to start index from 0 instead of 1.\n",
    "\n",
    "y_preds = make_predictions(X_test[10:20])\n",
    "preds = return_binary_class_labels(y_preds.to(device), threshold, pos_label=1)\n",
    "\n",
    "# print(\"y_preds = {}\".format(y_preds))\n",
    "print(\"Actual    Target Values : {}\".format([target_classes[target] for target in Y_test[10:20]]))\n",
    "print(\"Predicted Target Values : {}\".format([target_classes[target] for target in preds]))\n",
    "print(\"Predicted Probabilities : {}\".format(y_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "You have passed a list for output_names but the model seems to not have multiple outputs!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_784748/3331589336.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# print(X_test[:10])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mshap_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexplainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mshap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_plot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshap_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jovyan/envs/tox_21_qsar_modeling/lib/python3.7/site-packages/shap/explainers/_partition.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, max_evals, fixed_context, main_effects, error_bounds, batch_size, outputs, silent, *args)\u001b[0m\n\u001b[1;32m    135\u001b[0m         return super().__call__(\n\u001b[1;32m    136\u001b[0m             \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_evals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_evals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfixed_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfixed_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmain_effects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmain_effects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_bounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merror_bounds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m             \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msilent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msilent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m         )\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jovyan/envs/tox_21_qsar_modeling/lib/python3.7/site-packages/shap/explainers/_explainer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, max_evals, main_effects, error_bounds, batch_size, outputs, silent, *args, **kwargs)\u001b[0m\n\u001b[1;32m    306\u001b[0m                 \u001b[0msliced_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m             \u001b[0;32massert\u001b[0m \u001b[0moutput_indices\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"You have passed a list for output_names but the model seems to not have multiple outputs!\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    309\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m             \u001b[0msliced_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex_list\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mindex_list\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutput_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: You have passed a list for output_names but the model seems to not have multiple outputs!"
     ]
    }
   ],
   "source": [
    "# print(X_test[:10])\n",
    "shap_values = explainer(X_test[10:20])\n",
    "shap.text_plot(shap_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Average Word Embeddings\n",
    "In this section, we have used a little different approach compared to our previous two approaches. In the previous approach, we kept embeddings for all words/tokens of the text example by laying them next to each other, but in this section, we have taken the average of embeddings of all tokens/words per text example. The majority of the code is exactly the same as our previous sections with only a change in handling embedding.\\\n",
    "\n",
    "**Here are some good references about operations on word embeddings:**\n",
    "> - [Why is it Okay to Average Embeddings?](https://randorithms.com/2020/11/17/Adding-Embeddings.html)\n",
    "> - [Why does averaging word embedding vectors (exctracted from the NN embedding layer) work to represent sentences?](https://datascience.stackexchange.com/questions/107462/why-does-averaging-word-embedding-vectors-exctracted-from-the-nn-embedding-laye)\n",
    "> - [Word2vec Word Embedding Operations: Add, Concatenate or Average Word Vectors?](https://www.baeldung.com/cs/word2vec-word-embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivityNet(nn.Module):\n",
    "    def __init__(self, num_tokens, embedding_size, hidden_size, dropout_rate, output_size, squash_output=True):\n",
    "        super(ActivityNet, self).__init__()\n",
    "        \n",
    "        self.num_tokens     = num_tokens          \n",
    "        self.hidden_size    = hidden_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.dropout_rate   = dropout_rate\n",
    "        self.output_size    = output_size\n",
    "#         self.proj_size = proj_size\n",
    "        self.squash_output  = squash_output\n",
    "    \n",
    "        # Embedding Layer - The embedding layer has shape [num_tokens, embedding_size]. This will create a weight of the same shape \n",
    "        # hence each word will be mapped to embedding_size embeddings (a float vector of length embedding_size). \n",
    "        # The embedding layer takes a number in the range [0, num_tokens] as input and maps each one to their respective embeddings (float vectors). \n",
    "# #         print(\"{} - {}\".format(self.num_tokens, self.embedding_size))\n",
    "#         self.embedding = nn.EmbeddingBag(num_embeddings=self.num_tokens, embedding_dim=self.embedding_size,mode=\"mean\")\n",
    "        self.embedding = nn.Embedding(num_embeddings=self.num_tokens, embedding_dim=self.embedding_size)\n",
    "        \n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=self.embedding_size, hidden_size=self.hidden_size, num_layers=1\n",
    "                            , batch_first=True # If True, then the input and output tensors are provided as (batch, seq, feature) instead of (seq, batch, feature).\n",
    "                            , bidirectional=False # If True, becomes a bidirectional LSTM.\n",
    "#                             , proj_size=self.proj_size # If > 0, will use LSTM with projections of corresponding size.\n",
    "                            )\n",
    "        self.h1 = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "#         self.h2 = nn.Linear(hidden_size, hidden_size)\n",
    "#         self.h3 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.output_layer = nn.Linear(self.hidden_size, self.output_size)\n",
    "      \n",
    "        # Activation\n",
    "        self.hl_activation = nn.ReLU()\n",
    "        if self.squash_output:\n",
    "            self.ol_activation = nn.Sigmoid()\n",
    "        \n",
    "        # Regularization\n",
    "        self.dropout1 = nn.Dropout(self.dropout_rate)\n",
    "        self.dropout2 = nn.Dropout(self.dropout_rate)\n",
    "#         self.dropout3 = nn.Dropout(self.dropout_rate)\n",
    "       \n",
    "    def forward(self, X):        \n",
    "#         print(\"X class = {}\".format(X.__class__))\n",
    "        embeddings = self.embedding(X)\n",
    "\n",
    "#         print(embeddings.shape)\n",
    "#         print(embeddings[0])\n",
    "        embeddings = embeddings.mean(dim=1, keepdim=True)  ## Averaging embeddings\n",
    "#         print(embeddings.shape)\n",
    "#         print(embeddings[0])\n",
    "#         print(\"\\n\")\n",
    "\n",
    "        output, (h_n, c_n) = self.lstm(embeddings)\n",
    "#         print(\"output:{} h_n:{} c_n:{}\".format(output.shape, h_n.shape, c_n.shape))\n",
    "        output = self.dropout1(h_n) #Dropout\n",
    "        output = self.h1(output) # Pass into the hidden layer\n",
    "        output = self.hl_activation(output) # Use ReLU on hidden activation\n",
    "        output = self.dropout2(output) # dropout\n",
    "        \n",
    "#         output = self.h2(output)\n",
    "#         output = self.hl_activation(output)\n",
    "#         output = self.dropout3(output)\n",
    "        output = self.output_layer(output)\n",
    "\n",
    "        if self.squash_output:\n",
    "            output = self.ol_activation(output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(model, criterion, optimizer, train_loader, pos_label=1, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Fit the model, and compute loss and accuracy\n",
    "    \"\"\"\n",
    "    running_loss        = 0\n",
    "    num_samples         = 0\n",
    "    correct_predictions = 0\n",
    "    count = 0\n",
    "    model.train()\n",
    "    for smiles_rep, labels in train_loader:\n",
    "        smiles_rep = smiles_rep.to(device)\n",
    "        labels = labels.to(device)\n",
    "#         print(smiles_rep.shape)\n",
    "#         print(labels.shape)\n",
    "\n",
    "        # Training pass\n",
    "        # (1) Initialize the gradients, which will be recorded during the forward pass\n",
    "        optimizer.zero_grad()\n",
    "        # (2) Forward pass of the mini-batch\n",
    "        output = model(smiles_rep).flatten().float()\n",
    "#         print(\"Train output: {}\".format(output.shape))\n",
    "#         print(\"Train output: {}\".format(output.type()))\n",
    "#         print(\"Train output[:5]: {}\".format(output[:5]))\n",
    "\n",
    "        # (3) Computing the loss\n",
    "#         print(\"Labels size: {}\".format(labels.shape))\n",
    "#         print(\"Labels: {}\".format(labels.float().type()))\n",
    "#         print(\"Labels: {}\".format(labels[:5]))\n",
    "        loss = criterion(output, labels.float())\n",
    "#         print(\"Train Loss: {}\".format(loss))\n",
    "        # (4) Computes the gradient of current tensor w.r.t. graph leaves.\n",
    "        loss.backward()\n",
    "#         nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "        # (5) Optimize the weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss    += loss.item()          \n",
    "        num_samples     += labels.size()[0]\n",
    "        predicted_labels = return_binary_class_labels(output, threshold, pos_label)\n",
    "\n",
    "        correct_predictions += (predicted_labels == labels).sum().item()       \n",
    "\n",
    "    train_loss     = running_loss/len(train_loader)            \n",
    "    train_accuracy = correct_predictions/num_samples\n",
    "        \n",
    "    return  train_loss, train_accuracy\n",
    "\n",
    "\n",
    "def validate_model(model, criterion, optimizer, validation_loader, pos_label=1, threshold=0.5):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    running_loss        = 0\n",
    "    num_val_samples     = 0\n",
    "    correct_predictions = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for smiles_rep, labels in validation_loader:\n",
    "            smiles_rep = smiles_rep.to(device)\n",
    "            labels = labels.to(device)\n",
    "            output = model(smiles_rep).flatten().float()\n",
    "            \n",
    "#             print(\"Validation output: {}\".format(output.shape))\n",
    "#             print(\"Validation: {}\".format(output.type()))\n",
    "#             print(\"Validation[:5]: {}\".format(output[:5]))              \n",
    "#             print(\"Labels: {}\".format(labels.float().type()))\n",
    "#             print(\"Validation Labels: {}\".format(labels[:5]))   \n",
    "\n",
    "            loss = criterion(output, labels.float())\n",
    "            running_loss     += loss\n",
    "               \n",
    "            num_val_samples  += labels.size()[0]\n",
    "            predicted_labels  = return_binary_class_labels(output, threshold, pos_label)\n",
    "#             print(\"Predicted labels ({}) : {}\".format(predicted_labels.shape, predicted_labels[:5]))\n",
    "#             print(\"Labels : {}\".format(labels[:5]))\n",
    "#             print(\"predicted_labels == labels : {}\".format((predicted_.flatten()).sum().item()\n",
    "            correct_predictions += (predicted_labels == labels).sum().item()    \n",
    "        validation_loss     = running_loss/len(validation_loader)\n",
    "#         validation_loss     = (running_loss/len(validation_loader)).to('cpu')            \n",
    "        validation_accuracy = correct_predictions/num_val_samples   \n",
    "        \n",
    "    return validation_loss, validation_accuracy\n",
    "\n",
    "\n",
    "def train_model(model, criterion, optimizer, n_epochs, train_loader, validation_loader, lr_scheduler=None, early_stopper=None, with_proba=True, pos_label=1, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Fit and validate model\n",
    "    \"\"\"\n",
    "    train_losses, validation_losses, train_accuracies, validation_accuracies = [], [], [], []\n",
    "    for e in tqdm(range(n_epochs),desc='Training'):\n",
    "#         print(\"Epoch {} of {} epochs\".format(e+1, n_epochs))\n",
    "        train_loss, train_accuracy           = fit_model(model=model, criterion=criterion, optimizer=optimizer, train_loader=train_loader\n",
    "                                                         , pos_label=pos_label, threshold=threshold)\n",
    "        validation_loss, validation_accuracy = validate_model(model=model, criterion=criterion, optimizer=optimizer, validation_loader=validation_loader\n",
    "                                                              , pos_label=pos_label, threshold=threshold)\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        validation_losses.append(validation_loss.item())\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        validation_accuracies.append(validation_accuracy)\n",
    "        \n",
    "        if lr_scheduler is not None:\n",
    "            lr_scheduler.step(validation_loss)\n",
    "            \n",
    "        if early_stopper is not None:\n",
    "            early_stopper(validation_loss)\n",
    "            if early_stopper.early_stop:\n",
    "                break\n",
    "        \n",
    "        if e%10 == 0:\n",
    "            print(\"Epoch: %3i Training loss: %0.2F Validation loss: %0.2F\"%(e,(train_loss), validation_loss))\n",
    "            print(\"Epoch: %3i Training accuracy: %0.2F Validation accuracy: %0.2F\"%(e,(train_accuracy), validation_accuracy))\n",
    "    \n",
    "    return train_losses, validation_losses, train_accuracies, validation_accuracies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tokens=50\n",
    "# copied from https://coderzcolumn.com/tutorials/artificial-intelligence/word-embeddings-for-pytorch-text-classification-networks\n",
    "def vectorize_batch(batch):\n",
    "    Y, X = list(zip(*batch))\n",
    "    X = [tt_vocab(spe.tokenize(sample).split(' ')) for sample in X]\n",
    "#     print(len(X))\n",
    "#     print(Y.__class__)\n",
    "#     print(Y)\n",
    "    X = [sample+([0]* (max_tokens-len(sample))) if len(sample)<max_tokens else sample[:max_tokens] for sample in X] ## Bringing all samples to max_tokens length.\n",
    "    return torch.tensor(X), torch.tensor(Y) ## We have deducted 1 from target names to get them in range [0,1,2,3,5] from [1,2,3,4,5]\n",
    "\n",
    "train_dataset      = SMILESMolDataset(smiles=train_aug[smiles_column].values, target=train_aug[target].values)\n",
    "validation_dataset = SMILESMolDataset(smiles=validation_aug[smiles_column].values, target=validation_aug[target].values)\n",
    "test_dataset       = SMILESMolDataset(smiles=test_aug[smiles_column].values, target=test_aug[target].values)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, collate_fn=vectorize_batch)\n",
    "validation_loader  = DataLoader(validation_dataset, batch_size=64, collate_fn=vectorize_batch)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=64, collate_fn=vectorize_batch)\n",
    "\n",
    "# print(\"TRAIN ------------\")\n",
    "# for X, Y in train_loader:\n",
    "#     print(X)\n",
    "#     print(Y)\n",
    "#     break\n",
    "# print(\"\\nVALIDATION ------------\")\n",
    "# for X, Y in validation_loader:\n",
    "#     print(X)\n",
    "#     print(Y)\n",
    "#     break\n",
    "# print(\"\\nTEST ------------\")\n",
    "# for X, Y in test_loader:\n",
    "#     print(X)\n",
    "#     print(Y)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t = torch.tensor([ 0.0401,  0.3728,  0.8811,  0.1067, -0.4404, -1.6970, -1.6379,  0.4634, -0.0870\n",
    "#                   ,  1.0152,  0.4510, -0.2229, 0.3448, -0.1875, -0.6355, -1.0281, -0.6180\n",
    "#                   ,  0.1820,  0.7179, -0.3506,  1.0886,  1.0709, -0.4945, -0.4642, 0.3469])\n",
    "\n",
    "# t.size(), t.view(25,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "embedding_size = 40\n",
    "model        = ActivityNet(num_tokens, embedding_size, hidden_size, dropout_rate, output_size, squash_output=True).to(device=device) # Returns a Tensor with the specified device\n",
    "\n",
    "train_losses, validation_losses, train_accuracies, validation_accuracies = train_model(model=model, criterion=criterion, optimizer=optimizer, n_epochs=n_epochs\n",
    "                                                                                     , train_loader=train_loader, validation_loader=validation_loader\n",
    "                                                                                     , lr_scheduler=lr_scheduler, early_stopper=None\n",
    "                                                                                     , pos_label=1, threshold=threshold)\n",
    "\n",
    "print('\\nSaving loss and accuracy plots...')\n",
    "# accuracy plots\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.plot(train_accuracies, color='green', label='train accuracy')\n",
    "plt.plot(validation_accuracies, color='blue', label='validataion accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "# plt.legend()\n",
    "plt.savefig(\"{}/figures/Accuracies by epoch\".format(ROOT_DIR))\n",
    "plt.show()\n",
    "\n",
    "# loss plots\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.plot(train_losses, color='orange', label='train loss')\n",
    "plt.plot(validation_losses, color='red', label='validataion loss')\n",
    "# plt.plot([x for x in train_losses], color='orange', label='train loss')\n",
    "# plt.plot([x.to('cpu') for x in validation_losses], color='red', label='validataion loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.savefig(\"{}/figures/Losses by epoch\".format(ROOT_DIR))\n",
    "plt.show()\n",
    "\n",
    "# validation_losses[0].shape, len(validation_losses), train_losses[0].shape, len(train_losses)\n",
    "train_losses[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If the validation loss is generally smaller than the training loss, read the following post:**\n",
    ">[Why is my validation loss lower than my training loss?](https://pyimagesearch.com/2019/10/14/why-is-my-validation-loss-lower-than-my-training-loss/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import gc\n",
    "\n",
    "def MakePredictions(model, loader):\n",
    "    Y_shuffled, Y_preds = [], []\n",
    "    for X, Y in loader:\n",
    "        X = X.to(device)\n",
    "        Y = Y.to(device)\n",
    "        preds = model(X)[0]\n",
    "        pred_classes = return_binary_class_labels(preds, threshold, pos_label=1).flatten()\n",
    "#         print(\"preds = {} | labels = {}\".format(pred_classes.shape, Y.shape))\n",
    "#         print(\"Y = {}\".format(Y))\n",
    "#         print(\"pred_classes = {}\".format(pred_classes))\n",
    "       \n",
    "        Y_preds.append(pred_classes)\n",
    "        Y_shuffled.append(Y)\n",
    "    gc.collect()\n",
    "    Y_preds, Y_shuffled = torch.cat(Y_preds), torch.cat(Y_shuffled)\n",
    "\n",
    "    return Y_shuffled.detach().to('cpu').numpy(), Y_preds.detach().to('cpu').numpy()\n",
    "Y_actual, Y_preds = MakePredictions(model, test_loader)\n",
    "Y_actual.shape, Y_preds.shape #, Y_actual, Y_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "target_classes=['Inactive','Active']\n",
    "print(\"Test Accuracy : {}\".format(accuracy_score(Y_actual, Y_preds)))\n",
    "print(\"\\nClassification Report : \")\n",
    "print(classification_report(Y_actual, Y_preds, target_names=target_classes))\n",
    "print(\"\\nConfusion Matrix : \")\n",
    "print(confusion_matrix(Y_actual, Y_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import scikitplot as skplt\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "skplt.metrics.plot_confusion_matrix([target_classes[i] for i in Y_actual], [target_classes[i] for i in Y_preds],\n",
    "                                    normalize=True,\n",
    "                                    title=\"Confusion Matrix\",\n",
    "                                    cmap=\"Purples\",\n",
    "                                    hide_zeros=True,\n",
    "                                    figsize=(5,5)\n",
    "                                    );\n",
    "plt.xticks(rotation=90);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explain Predictions Using SHAP Values\n",
    "In this section, we have tried to explain predictions made by our network by generating **SHAP** values using shap python library. In order to use shap library, we need to import it and initialize it by calling **initjs()** function on it which we did at the beginning of the tutorial.\n",
    "\n",
    "In order to explain a prediction using SHAP values, we need to create **Explainer** first. Then, we need to give text examples that we want to explain to the explainer instance to create **Explanation** object (SHAP values). At last, we can call **text_plot()** method by giving SHAP values to it to visualize explanations created for text examples.\n",
    "\n",
    "Below, we have first created an explainer object. The explainer object requires us to provide a function that takes as an input batch of text examples and returns probabilities for each target class for the whole batch. We have created a simple function that takes as an input batch of text samples. It then tokenizes them and creates indexes for tokens using vocabulary. It then assures that each text sample has a length of ***max_tokens*** as required by our network. Then, it gives vectorized batch data to the network to make predictions. As our network returns logits, we have converted them to probabilities using **softmax** activation function. We have also given target class names when creating explainer instances.\n",
    "\n",
    "If you do not have a background on SHAP library then we suggest the below-mentioned tutorials that can be very helpful to learn it.\n",
    "> - [SHAP - Explain Machine Learning Model Predictions using Game-Theoretic Approach](https://coderzcolumn.com/tutorials/machine-learning/shap-explain-machine-learning-model-predictions-using-game-theoretic-approach)\n",
    "> - [SHAP Values for Text Classification Tasks (Keras NLP)](https://coderzcolumn.com/tutorials/artificial-intelligence/shap-values-for-text-classification-tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, Y_test = [], []\n",
    "for Y, X in test_dataset: ## Selecting first 1024 samples from test data\n",
    "    X_test.append(X)\n",
    "    Y_test.append(Y) ## Please make a Note that we have subtracted 1 from target values to start index from 0 instead of 1.\n",
    "\n",
    "y_preds = make_predictions(X_test[:10])\n",
    "preds = return_binary_class_labels(y_preds.to(device), threshold, pos_label=1).flatten()\n",
    "\n",
    "# print(\"y_preds = {}\".format(y_preds))\n",
    "print(\"Actual    Target Values : {}\".format([target_classes[target] for target in Y_test[:10]]))\n",
    "print(\"Predicted Target Values : {}\".format([target_classes[target] for target in preds]))\n",
    "print(\"Predicted Probabilities : {}\".format([round(x.item(),3) for x in y_preds.flatten()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_test[:10])\n",
    "shap_values = explainer(X_test[:10])\n",
    "shap.text_plot(shap_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using EmbeddingBag Layer (Averaged Embeddings)\n",
    "Our approach in this section is almost the same as our approach from the previous section with the only difference that we have implemented the approach using **EmbeddingBag** layer. We have again averaged embeddings of text examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivityNet(nn.Module):\n",
    "    def __init__(self, num_tokens, embedding_size, hidden_size, dropout_rate, output_size, squash_output=True):\n",
    "        super(ActivityNet, self).__init__()\n",
    "        \n",
    "        self.num_tokens     = num_tokens          \n",
    "        self.hidden_size    = hidden_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.dropout_rate   = dropout_rate\n",
    "        self.output_size    = output_size\n",
    "#         self.proj_size = proj_size\n",
    "        self.squash_output  = squash_output\n",
    "    \n",
    "        # Embedding Layer - The embedding layer has shape [num_tokens, embedding_size]. This will create a weight of the same shape \n",
    "        # hence each word will be mapped to embedding_size embeddings (a float vector of length embedding_size). \n",
    "        # The embedding layer takes a number in the range [0, num_tokens] as input and maps each one to their respective embeddings (float vectors). \n",
    "# #         print(\"{} - {}\".format(self.num_tokens, self.embedding_size))\n",
    "        self.embedding = nn.EmbeddingBag(num_embeddings=self.num_tokens, embedding_dim=self.embedding_size,mode=\"sum\")\n",
    "#         self.embedding = nn.Embedding(num_embeddings=self.num_tokens, embedding_dim=self.embedding_size)\n",
    "        \n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=self.embedding_size, hidden_size=self.hidden_size, num_layers=1\n",
    "                            , batch_first=True # If True, then the input and output tensors are provided as (batch, seq, feature) instead of (seq, batch, feature).\n",
    "                            , bidirectional=False # If True, becomes a bidirectional LSTM.\n",
    "#                             , proj_size=self.proj_size # If > 0, will use LSTM with projections of corresponding size.\n",
    "                            )\n",
    "        self.h1 = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "#         self.h2 = nn.Linear(hidden_size, hidden_size)\n",
    "#         self.h3 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.output_layer = nn.Linear(self.hidden_size, self.output_size)\n",
    "      \n",
    "        # Activation\n",
    "        self.hl_activation = nn.ReLU()\n",
    "        if self.squash_output:\n",
    "            self.ol_activation = nn.Sigmoid()\n",
    "        \n",
    "        # Regularization\n",
    "        self.dropout1 = nn.Dropout(self.dropout_rate)\n",
    "        self.dropout2 = nn.Dropout(self.dropout_rate)\n",
    "#         self.dropout3 = nn.Dropout(self.dropout_rate)\n",
    "       \n",
    "    def forward(self, X):        \n",
    "#         print(\"X class = {}\".format(X.__class__))\n",
    "        embeddings = self.embedding(X)\n",
    "\n",
    "#         print(embeddings.shape)\n",
    "#         print(embeddings[0])\n",
    "        embeddings=embeddings.view(embeddings.shape[0],1,embeddings.shape[1])\n",
    "#         print(bla)\n",
    "#         print(bla.shape)\n",
    "#         embeddings = embeddings.mean(dim=1, keepdim=True)  ## Averaging embeddings\n",
    "#         print(embeddings.shape)\n",
    "#         print(embeddings[0])\n",
    "#         print(\"\\n\")\n",
    "        \n",
    "#         print(self.lstm(embeddings))\n",
    "        output, (h_n, c_n) = self.lstm(embeddings)\n",
    "#         print(\"output:{} h_n:{} c_n:{}\".format(output.shape, h_n.shape, c_n.shape))\n",
    "#         print(self.dropout1)\n",
    "        output = self.dropout1(h_n) #Dropout\n",
    "        output = self.h1(output) # Pass into the hidden layer\n",
    "        output = self.hl_activation(output) # Use ReLU on hidden activation\n",
    "        output = self.dropout2(output) # dropout\n",
    "        \n",
    "#         output = self.h2(output)\n",
    "#         output = self.hl_activation(output)\n",
    "#         output = self.dropout3(output)\n",
    "        output = self.output_layer(output)\n",
    "\n",
    "        if self.squash_output:\n",
    "            output = self.ol_activation(output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(model, criterion, optimizer, train_loader, pos_label=1, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Fit the model, and compute loss and accuracy\n",
    "    \"\"\"\n",
    "    running_loss        = 0\n",
    "    num_samples         = 0\n",
    "    correct_predictions = 0\n",
    "    count = 0\n",
    "    model.train()\n",
    "    for smiles_rep, labels in train_loader:\n",
    "        smiles_rep = smiles_rep.to(device)\n",
    "        labels = labels.to(device)\n",
    "#         print(smiles_rep.shape)\n",
    "#         print(labels.shape)\n",
    "\n",
    "        # Training pass\n",
    "        # (1) Initialize the gradients, which will be recorded during the forward pass\n",
    "        optimizer.zero_grad()\n",
    "        # (2) Forward pass of the mini-batch\n",
    "        output = model(smiles_rep).flatten().float()\n",
    "#         print(\"Train output: {}\".format(output.shape))\n",
    "#         print(\"Train output: {}\".format(output.type()))\n",
    "#         print(\"Train output[:5]: {}\".format(output[:5]))\n",
    "\n",
    "        # (3) Computing the loss\n",
    "#         print(\"Labels size: {}\".format(labels.shape))\n",
    "#         print(\"Labels: {}\".format(labels.float().type()))\n",
    "#         print(\"Labels: {}\".format(labels[:5]))\n",
    "        loss = criterion(output, labels.float())\n",
    "#         print(\"Train Loss: {}\".format(loss))\n",
    "        # (4) Computes the gradient of current tensor w.r.t. graph leaves.\n",
    "        loss.backward()\n",
    "#         nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "        # (5) Optimize the weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss    += loss.item()          \n",
    "        num_samples     += labels.size()[0]\n",
    "        predicted_labels = return_binary_class_labels(output, threshold, pos_label)\n",
    "\n",
    "        correct_predictions += (predicted_labels == labels).sum().item()       \n",
    "\n",
    "    train_loss     = running_loss/len(train_loader)            \n",
    "    train_accuracy = correct_predictions/num_samples\n",
    "        \n",
    "    return  train_loss, train_accuracy\n",
    "\n",
    "\n",
    "def validate_model(model, criterion, optimizer, validation_loader, pos_label=1, threshold=0.5):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    running_loss        = 0\n",
    "    num_val_samples     = 0\n",
    "    correct_predictions = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for smiles_rep, labels in validation_loader:\n",
    "            smiles_rep = smiles_rep.to(device)\n",
    "            labels = labels.to(device)\n",
    "            output = model(smiles_rep).flatten().float()\n",
    "            \n",
    "#             print(\"Validation output: {}\".format(output.shape))\n",
    "#             print(\"Validation: {}\".format(output.type()))\n",
    "#             print(\"Validation[:5]: {}\".format(output[:5]))              \n",
    "#             print(\"Labels: {}\".format(labels.float().type()))\n",
    "#             print(\"Labels: {}\".format(labels[:5]))   \n",
    "\n",
    "            loss = criterion(output, labels.float())\n",
    "            running_loss     += loss\n",
    "               \n",
    "            num_val_samples  += labels.size()[0]\n",
    "            predicted_labels  = return_binary_class_labels(output, threshold, pos_label)\n",
    "#             print(\"Predicted labels ({}) : {}\".format(predicted_labels.shape, predicted_labels[:5]))\n",
    "#             print(\"Labels : {}\".format(labels[:5]))\n",
    "#             print(\"predicted_labels == labels : {}\".format((predicted_.flatten()).sum().item()\n",
    "            correct_predictions += (predicted_labels == labels).sum().item()    \n",
    "        validation_loss     = running_loss/len(validation_loader)\n",
    "#         validation_loss     = (running_loss/len(validation_loader)).to('cpu')            \n",
    "        validation_accuracy = correct_predictions/num_val_samples   \n",
    "        \n",
    "    return validation_loss, validation_accuracy\n",
    "\n",
    "\n",
    "def train_model(model, criterion, optimizer, n_epochs, train_loader, validation_loader, lr_scheduler=None, early_stopper=None, with_proba=True, pos_label=1, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Fit and validate model\n",
    "    \"\"\"\n",
    "    train_losses, validation_losses, train_accuracies, validation_accuracies = [], [], [], []\n",
    "    for e in tqdm(range(n_epochs),desc='Training'):\n",
    "#         print(\"Epoch {} of {} epochs\".format(e+1, n_epochs))\n",
    "        train_loss, train_accuracy           = fit_model(model=model, criterion=criterion, optimizer=optimizer, train_loader=train_loader\n",
    "                                                         , pos_label=pos_label, threshold=threshold)\n",
    "        validation_loss, validation_accuracy = validate_model(model=model, criterion=criterion, optimizer=optimizer, validation_loader=validation_loader\n",
    "                                                              , pos_label=pos_label, threshold=threshold)\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        validation_losses.append(validation_loss.item())\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        validation_accuracies.append(validation_accuracy)\n",
    "        \n",
    "        if lr_scheduler is not None:\n",
    "            lr_scheduler.step(validation_loss)\n",
    "            \n",
    "        if early_stopper is not None:\n",
    "            early_stopper(validation_loss)\n",
    "            if early_stopper.early_stop:\n",
    "                break\n",
    "        \n",
    "        if e%10 == 0:\n",
    "            print(\"Epoch: %3i Training loss: %0.2F Validation loss: %0.2F\"%(e,(train_loss), validation_loss))\n",
    "            print(\"Epoch: %3i Training accuracy: %0.2F Validation accuracy: %0.2F\"%(e,(train_accuracy), validation_accuracy))\n",
    "    \n",
    "    return train_losses, validation_losses, train_accuracies, validation_accuracies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "embedding_size = 40\n",
    "model        = ActivityNet(num_tokens, embedding_size, hidden_size, dropout_rate, output_size, squash_output=False).to(device=device) # Returns a Tensor with the specified device\n",
    "\n",
    "train_losses, validation_losses, train_accuracies, validation_accuracies = train_model(model=model, criterion=criterion, optimizer=optimizer, n_epochs=n_epochs\n",
    "                                                                                     , train_loader=train_loader, validation_loader=validation_loader\n",
    "                                                                                     , lr_scheduler=lr_scheduler, early_stopper=None\n",
    "                                                                                     , pos_label=1, threshold=threshold)\n",
    "\n",
    "print('\\nSaving loss and accuracy plots...')\n",
    "# accuracy plots\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.plot(train_accuracies, color='green', label='train accuracy')\n",
    "plt.plot(validation_accuracies, color='blue', label='validataion accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "# plt.legend()\n",
    "plt.savefig(\"{}/figures/Accuracies by epoch\".format(ROOT_DIR))\n",
    "plt.show()\n",
    "\n",
    "# loss plots\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.plot(train_losses, color='orange', label='train loss')\n",
    "plt.plot(validation_losses, color='red', label='validataion loss')\n",
    "# plt.plot([x for x in train_losses], color='orange', label='train loss')\n",
    "# plt.plot([x.to('cpu') for x in validation_losses], color='red', label='validataion loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.savefig(\"{}/figures/Losses by epoch\".format(ROOT_DIR))\n",
    "plt.show()\n",
    "\n",
    "# validation_losses[0].shape, len(validation_losses), train_losses[0].shape, len(train_losses)\n",
    "train_losses[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "target_classes=['Inactive','Active']\n",
    "print(\"Test Accuracy : {}\".format(accuracy_score(Y_actual, Y_preds)))\n",
    "print(\"\\nClassification Report : \")\n",
    "print(classification_report(Y_actual, Y_preds, target_names=target_classes))\n",
    "print(\"\\nConfusion Matrix : \")\n",
    "print(confusion_matrix(Y_actual, Y_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import scikitplot as skplt\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "skplt.metrics.plot_confusion_matrix([target_classes[i] for i in Y_actual], [target_classes[i] for i in Y_preds],\n",
    "                                    normalize=True,\n",
    "                                    title=\"Confusion Matrix\",\n",
    "                                    cmap=\"Purples\",\n",
    "                                    hide_zeros=True,\n",
    "                                    figsize=(5,5)\n",
    "                                    );\n",
    "plt.xticks(rotation=90);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explain Predictions Using SHAP Values\n",
    "In this section, we have tried to explain predictions made by our network by generating **SHAP** values using shap python library. In order to use shap library, we need to import it and initialize it by calling **initjs()** function on it which we did at the beginning of the tutorial.\n",
    "\n",
    "In order to explain a prediction using SHAP values, we need to create **Explainer** first. Then, we need to give text examples that we want to explain to the explainer instance to create **Explanation** object (SHAP values). At last, we can call **text_plot()** method by giving SHAP values to it to visualize explanations created for text examples.\n",
    "\n",
    "Below, we have first created an explainer object. The explainer object requires us to provide a function that takes as an input batch of text examples and returns probabilities for each target class for the whole batch. We have created a simple function that takes as an input batch of text samples. It then tokenizes them and creates indexes for tokens using vocabulary. It then assures that each text sample has a length of ***max_tokens*** as required by our network. Then, it gives vectorized batch data to the network to make predictions. As our network returns logits, we have converted them to probabilities using **softmax** activation function. We have also given target class names when creating explainer instances.\n",
    "\n",
    "If you do not have a background on SHAP library then we suggest the below-mentioned tutorials that can be very helpful to learn it.\n",
    "> - [SHAP - Explain Machine Learning Model Predictions using Game-Theoretic Approach](https://coderzcolumn.com/tutorials/machine-learning/shap-explain-machine-learning-model-predictions-using-game-theoretic-approach)\n",
    "> - [SHAP Values for Text Classification Tasks (Keras NLP)](https://coderzcolumn.com/tutorials/artificial-intelligence/shap-values-for-text-classification-tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, Y_test = [], []\n",
    "for Y, X in test_dataset: ## Selecting first 1024 samples from test data\n",
    "    X_test.append(X)\n",
    "    Y_test.append(Y) ## Please make a Note that we have subtracted 1 from target values to start index from 0 instead of 1.\n",
    "\n",
    "y_preds = make_predictions(X_test[:10])\n",
    "preds = return_binary_class_labels(y_preds.to(device), threshold, pos_label=1).flatten()\n",
    "\n",
    "# print(\"y_preds = {}\".format(y_preds))\n",
    "print(\"Actual    Target Values : {}\".format([target_classes[target] for target in Y_test[:10]]))\n",
    "print(\"Predicted Target Values : {}\".format([target_classes[target] for target in preds]))\n",
    "print(\"Predicted Probabilities : {}\".format([round(x.item(),3) for x in y_preds.flatten()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_test[:10])\n",
    "shap_values = explainer(X_test[:10])\n",
    "shap.text_plot(shap_values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:jovyan-tox_21_qsar_modeling]",
   "language": "python",
   "name": "conda-env-jovyan-tox_21_qsar_modeling-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
